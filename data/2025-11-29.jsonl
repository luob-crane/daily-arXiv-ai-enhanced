{"id": "2511.20709", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.20709", "abs": "https://arxiv.org/abs/2511.20709", "authors": ["Abhijeet Pathak", "Suvadra Barua", "Dinesh Gudimetla", "Rupam Patir", "Jiawei Guo", "Hongxin Hu", "Haipeng Cai"], "title": "DUALGUAGE: Automated Joint Security-Functionality Benchmarking for Secure Code Generation", "comment": null, "summary": "Large language models (LLMs) and autonomous coding agents are increasingly used to generate software across a wide range of domains. Yet a core requirement remains unmet: ensuring that generated code is secure without compromising its functional correctness. Existing benchmarks and evaluations for secure code generation fall short-many measure only vulnerability reduction, disregard correctness preservation, or evaluate security and functionality on separate datasets, violating the fundamental need for simultaneous joint evaluation. We present DUALGAUGE, the first fully automated benchmarking framework designed to rigorously evaluate the security and correctness of LLM-generated code in unison. Given the lack of datasets enabling joint evaluation of secure code generation, we also present DUALGAUGE-BENCH, a curated benchmark suite of diverse coding tasks, each paired with manually validated test suites for both security and functionality, designed for full coverage of specification requirements. At the core of DUALGAUGE is an agentic program executor, which runs a program against given tests in sandboxed environments, and an LLM-based evaluator, which assesses both correctness and vulnerability behavior against expected outcomes. We rigorously evaluated and ensured the quality of DUALGAUGE-BENCH and the accuracy of DUALGAUGE, and applied DUALGAUGE to benchmarking ten leading LLMs on DUALGAUGE-BENCH across thousands of test scenarios. Our results reveal critical gaps in correct and secure code generation by these LLMs, for which our open-source system and datasets help accelerate progress via reproducible, scalable, and rigorous evaluation."}
{"id": "2511.20730", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20730", "abs": "https://arxiv.org/abs/2511.20730", "authors": ["Nehal Afifi", "Christoph Wittig", "Lukas Paehler", "Andreas Lindenmann", "Kai Wolter", "Felix Leitenberger", "Melih Dogru", "Patric Grauberger", "Tobias Düser", "Albert Albers", "Sven Matthiesen"], "title": "Data-Driven Methods and AI in Engineering Design: A Systematic Literature Review Focusing on Challenges and Opportunities", "comment": null, "summary": "The increasing availability of data and advancements in computational intelligence have accelerated the adoption of data-driven methods (DDMs) in product development. However, their integration into product development remains fragmented. This fragmentation stems from uncertainty, particularly the lack of clarity on what types of DDMs to use and when to employ them across the product development lifecycle. To address this, a necessary first step is to investigate the usage of DDM in engineering design by identifying which methods are being used, at which development stages, and for what application. This paper presents a PRISMA systematic literature review. The V-model as a product development framework was adopted and simplified into four stages: system design, system implementation, system integration, and validation. A structured search across Scopus, Web of Science, and IEEE Xplore (2014--2024) retrieved 1{,}689 records. After screening, 114 publications underwent full-text analysis. Findings show that machine learning (ML) and statistical methods dominate current practice, whereas deep learning (DL), though still less common, exhibits a clear upward trend in adoption. Additionally, supervised learning, clustering, regression analysis, and surrogate modeling are prevalent in design, implementation, and integration system stages but contributions to validation remain limited. Key challenges in existing applications include limited model interpretability, poor cross-stage traceability, and insufficient validation under real-world conditions. Additionally, it highlights key limitations and opportunities such as the need for interpretable hybrid models. This review is a first step toward design-stage guidelines; a follow-up synthesis should map computer science algorithms to engineering design problems and activities."}
{"id": "2511.20813", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.20813", "abs": "https://arxiv.org/abs/2511.20813", "authors": ["Simon Hacks"], "title": "Train While You Fight -- Technical Requirements for Advanced Distributed Learning Platforms", "comment": "17 pages, submitted to CAiSE - International Conference on Advanced information Systems Engineering 2026", "summary": "\"Train While You Fight\" (TWYF) advocates for continuous learning that occurs during operations, not just before or after. This paper examines the technical requirements that advanced distributed learning (ADL) platforms must meet to support TWYF, and how existing software engineering patterns can fulfill these requirements. Using a Design Science Research approach, we (i) derive challenges from PfPC/NATO documentation and recent practice, (ii) define solution objectives, and (iii) conduct a systematic mapping from challenges to proven patterns. We identify seven technical challenges: interoperability, resilience, multilingual support, data security and privacy, scalability, platform independence, and modularity. We illustrate the patterns with a national use case from the German armed forces."}
{"id": "2511.20916", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.20916", "abs": "https://arxiv.org/abs/2511.20916", "authors": ["Illia Khudiakov", "Vladyslav Pliuhin", "Sergiy Plankovskyy", "Yevgen Tsegelnyk"], "title": "Application of machine learning for infrastructure reconstruction programs management", "comment": "8 pages, 2 figures, 3 tables", "summary": "The purpose of this article is to describe an adaptive decision-making support model aimed at improving the efficiency of engineering infrastructure reconstruction program management in the context of developing the architecture and work breakdown structure of programs. As part of the study, the existing adaptive program management tools are analyzed, the use of infrastructure systems modelling tools is justified for program architecture and WBS creation. Existing models and modelling methods are viewed, and machine learning and artificial neural networks are selected for the model. The main components of the model are defined, which include a set of decision-maker preferences, decision-making tasks, sets of input data, and applied software components of the model. To support decision-making, the adaptive model applies the method of system modeling and predicting the value of the objective function at a given system configuration. Prediction is done using machine learning methods based on a dataset consisting of historical data related to existing engineering systems. The work describes the components of the redistribution of varied model parameters, which modify the model dataset based on the selected object type, which allows adapting the decision-making process to the existing program implementation goals. The functional composition done in Microsoft Azure Machine Learning Studio is described. The neural network parameters and evaluation results are given. The application of the developed adaptive model is possible in the management of programs for the reconstruction of such engineering systems as systems of heat, gas, electricity supply, water supply, and drainage, etc."}
{"id": "2511.20862", "categories": ["cs.DL"], "pdf": "https://arxiv.org/pdf/2511.20862", "abs": "https://arxiv.org/abs/2511.20862", "authors": ["Myroslava Hladchenko"], "title": "Access models, authorship patterns, and citation impact in Ukrainian scholarly publishing (2020-2023)", "comment": null, "summary": "This study aimed to explore the relationship between access models, authorship patterns, and citation impact in Ukrainian research output from 2020 to 2023. The focus was on scholars affiliated with the National Academy of Sciences of Ukraine (NASU) and universities. Findings highlight that open access (OA) articles constituted the majority of publications by Ukrainian scholars during this period. This percentage reached 75.4% for NASU and 85.8% for universities. In both cases, the increase was driven by Gold OA and Hybrid Gold OA, the latter benefiting in part from Elsevier's waivers. Diamond OA prevailed for NASU, while Gold OA was dominant for universities. The effects of Russia's full-scale invasion of Ukraine included (1) a decline in the share of articles in foreign journals for both NASU and universities, (2) a decrease in Gold OA in foreign journals and an increase in Gold OA in Ukrainian journals for universities, and (3) a rise in internationally co-authored Gold OA articles in foreign journals for both entities. Despite waivers for Gold OA provided by major publishers and an increase in Gold OA articles in Elsevier and Springer journals, MDPI and Aluna Publishing House remained the dominant publishers of Gold OA in foreign journals. Hybrid Gold OA, Bronze OA, and Green OA articles in foreign journals had the highest citation impact. The citation impact of Gold OA outperformed Diamond OA. The study confirms the growing dominance of Gold OA, suggesting the need for sustainable OA models that ensure both equity and broad dissemination of research."}
{"id": "2511.21069", "categories": ["nucl-th"], "pdf": "https://arxiv.org/pdf/2511.21069", "abs": "https://arxiv.org/abs/2511.21069", "authors": ["Andrius Burnelis", "Daniel R. Phillips"], "title": "Simultaneous Inference of Effective Range Parameters and EFT Truncation Uncertainty in $^{3}$He-$α$ Scattering", "comment": "21 pages, 14 figures", "summary": "We extend previous halo effective field theory analyses of low-energy elastic scattering of $^{3}$He-$^{4}$He, including the $\\frac{7}{2}^{-}$ $f$-wave resonance as an explicit degree of freedom. The presence of this resonance necessitates a changing power counting scheme depending on the kinematic region. Therefore, we construct a theory uncertainty model at the partial wave amplitude level, allowing us to generate a sophisticated theory covariance matrix that captures the way the theory error structure changes as energy increases. We then perform a Bayesian analysis and simultaneously estimate the joint posterior distributions of the effective range theory parameters and the parameters that characterize the effective field theory truncation uncertainty. We compare two different analyses: no $f$-wave interactions for data up to $E_{\\text{max}} = 2.6$ MeV, and including $f$-wave interactions for data up to $E_{\\text{max}} = 5.5$ MeV. The inferred breakdown scales in each analysis are consistent with previous work. We find that $f$-wave interactions are needed to describe data for $E_{lab} \\gtrapprox 3.6$ MeV."}
{"id": "2511.20850", "categories": ["nucl-ex"], "pdf": "https://arxiv.org/pdf/2511.20850", "abs": "https://arxiv.org/abs/2511.20850", "authors": ["A. Tsantiri", "A. Spyrou", "E. C. Good", "K. Bosmpotinis", "P. Giuliani", "H. Arora", "G. Balk", "L. Balliet", "H. C. Berg", "J. M. Berkman", "C. Dembski", "P. DeYoung", "P. A. Denissenkov", "N. Dimitrakopoulos", "A. Doetsch", "T. Gaballah", "R. Garg", "A. Henriques", "R. Jain", "S. N. Liddick", "S. Lyons", "R. S. Lubna", "B. Monteagudo Godoy", "F. Montes", "S. Nash", "G. U. Ogudoro", "J. Owens-Fryar", "A. Palmisano-Kyle", "J. Pereira", "A. Psaltis", "A. L. Richard", "L. Roberti", "E. K. Ronning", "H. Schatz", "A. Sebastian", "M. Smith", "M. K. Smith", "C. S. Sumithrarachchi", "C. Tinson", "P. Tsintari", "N. Tubaro", "S. Uthayakumaar", "A. C. C. Villari", "E. Weissling", "R. G. T. Zegers"], "title": "Constraining the Synthesis of the Lightest p Nucleus 74Se", "comment": "9 pages, 4 figures, 1 table, published in Physical Review Letters as DOI: https://doi.org/10.1103/d7dr-h36j", "summary": "We provide the first experimental cross section of the $^{73}\\text{As}(p,γ)^{74}\\text{Se}$ reaction to constrain one of the main destruction mechanisms of the p nucleus $^{74}\\text{Se}$ in explosive stellar environments. The measurement was done using a radioactive $^{73}\\text{As}$ beam at effective center-of-mass energies of 2.9 and 2.3 MeV/nucleon. Along with the total cross-section measurement, statistical properties of the $^{74}\\text{Se}$ compound nucleus were extracted, constraining the reaction cross section in the upper Gamow window of the $γ$ process. The impact of the experimentally constrained reaction rate on $^{74}\\text{Se}$ production in Type II supernovae was investigated through Monte Carlo one-zone network simulations. The results indicate that the overproduction of $^{74}$Se by Type II supernova models cannot be resolved by nuclear physics alone and point toward the need for a more detailed understanding of the astrophysical conditions of relevance for the $γ$ process."}
{"id": "2511.20838", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.20838", "abs": "https://arxiv.org/abs/2511.20838", "authors": ["Amy K. Strong", "Leila Bridgeman"], "title": "Local Dissipativity Analysis of Nonlinear Systems", "comment": null, "summary": "Dissipativity is an input-output (IO) characterization of nonlinear systems that enables compositional robust control through Vidyasagar's Network Dissipativity Theorem. However, determining the dissipativity of a system is an involved and, often, model-specific process. We present a general method to determine the local dissipativity properties of nonlinear, control affine systems. We simultaneously search for the optimal IO characterization of a system and synthesize a continuous piecewise affine (CPA) storage function via a convex optimization problem. To do so, we reformulate the relationship between the Hamilton-Jacobi inequality and the dissipation inequality as an linear matrix inequality (LMI) and develop novel LMI bounds for a triangulation. Further, we develop a method to synthesize a combined quadratic and CPA storage function to expand the systems the optimization problem is applicable to. Finally, we demonstrate that our method will always find a feasible IO characterization and a CPA or quadratic storage function given that the system is strictly locally dissipative."}
{"id": "2511.20867", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.20867", "abs": "https://arxiv.org/abs/2511.20867", "authors": ["Puneet S. Bagga", "Vivek F. Farias", "Tamar Korkotashvili", "Tianyi Peng", "Yuhang Wu"], "title": "E-GEO: A Testbed for Generative Engine Optimization in E-Commerce", "comment": null, "summary": "With the rise of large language models (LLMs), generative engines are becoming powerful alternatives to traditional search, reshaping retrieval tasks. In e-commerce, for instance, conversational shopping agents now guide consumers to relevant products. This shift has created the need for generative engine optimization (GEO)--improving content visibility and relevance for generative engines. Yet despite its growing importance, current GEO practices are ad hoc, and their impacts remain poorly understood, especially in e-commerce. We address this gap by introducing E-GEO, the first benchmark built specifically for e-commerce GEO. E-GEO contains over 7,000 realistic, multi-sentence consumer product queries paired with relevant listings, capturing rich intent, constraints, preferences, and shopping contexts that existing datasets largely miss. Using this benchmark, we conduct the first large-scale empirical study of e-commerce GEO, evaluating 15 common rewriting heuristics and comparing their empirical performance. To move beyond heuristics, we further formulate GEO as a tractable optimization problem and develop a lightweight iterative prompt-optimization algorithm that can significantly outperform these baselines. Surprisingly, the optimized prompts reveal a stable, domain-agnostic pattern--suggesting the existence of a \"universally effective\" GEO strategy. Our data and code are publicly available at https://github.com/psbagga17/E-GEO."}
{"id": "2511.20780", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.20780", "abs": "https://arxiv.org/abs/2511.20780", "authors": ["Alison Silva", "Gustavo Callou"], "title": "Assessing Redundancy Strategies to Improve Availability in Virtualized System Architectures", "comment": null, "summary": "Cloud-based storage platforms are becoming more common in both academic and business settings due to their flexible access to data and support for collaborative functionalities. As reliability becomes a vital requirement, particularly for organizations looking for alternatives to public cloud services, assessing the dependability of these systems is crucial. This paper presents a methodology for analyzing the availability of a file server (Nextcloud) hosted in a private cloud environment using Apache CloudStack. The analysis is based on a modeling approach through Stochastic Petri Nets (SPNs) that allows the evaluation of different redundancy strategies to enhance the availability of such systems. Four architectural configurations were modeled, including the baseline, host-level redundancy, virtual machine (VM) redundancy, and a combination of both. The results show that redundancy at both the host and VM levels significantly improves availability and reduces expected downtime. The proposed approach provides a method to evaluate the availability of a private cloud and support infrastructure design decisions."}
{"id": "2511.20933", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.20933", "abs": "https://arxiv.org/abs/2511.20933", "authors": ["Mootez Saad", "Boqi Chen", "José Antonio Hernández López", "Dániel Varró", "Tushar Sharma"], "title": "Hierarchical Evaluation of Software Design Capabilities of Large Language Models of Code", "comment": "18 figures", "summary": "Large language models (LLMs) are being increasingly adopted in the software engineering domain, yet the robustness of their grasp on core software design concepts remains unclear. We conduct an empirical study to systematically evaluate their understanding of cohesion (intra-module) and coupling (inter-module). We programmatically generate poorly designed code fragments and test the DeepSeek-R1 model family ($14$B, $32$B, $70$B) under varying levels of guidance, from simple \\textit{Verification} to \\textit{Guided} and \\textit{Open-ended Generation}, while varying contextual noise by injecting distractor elements. While models exhibit a solid baseline understanding of both concepts in ideal conditions, their practical knowledge is fragile and highly asymmetrical. Reasoning about coupling proves brittle; performance collapses in noisy, open-ended scenarios, with F1 scores dropping by over $50\\%$. In contrast, the models' analysis of cohesion is remarkably robust to internal noise in guided tasks, showing little performance degradation. However, this resilience also fails when all guidance is removed. Reasoning-trace analysis confirms these failure modes, revealing \\textit{cognitive shortcutting} for coupling versus a more exhaustive (yet still failing) analysis for cohesion. To summarize, while LLMs can provide reliable assistance for recognizing design flaws, their ability to reason autonomously in noisy, realistic contexts is limited, highlighting the critical need for more scalable and robust program understanding capabilities."}
{"id": "2511.21176", "categories": ["cs.DL"], "pdf": "https://arxiv.org/pdf/2511.21176", "abs": "https://arxiv.org/abs/2511.21176", "authors": ["Zhengyi Zhou", "Ying Lou", "Zhesi Shen", "Menghui Li"], "title": "Prevalence and Trends in Global Retractions Explored Through a Topic Lens", "comment": "15 pages,11 figures", "summary": "Scientific publications form the cornerstone of innovation and have maintained a stable growth trend over the years. However, in recent years, there has been a significant surge in retractions, driven largely by the proliferation of low-quality and fraudulent papers. This study aims to examine retractions and their evolving trends through a topic lens. Our analysis of global retraction data reveals that the numbers of retraction have remained alarmingly high in recent years, with the growth rate of retracted papers significantly outpacing that of overall global publications. While retractions are observed across various fields, their distribution is not uniform. In disciplines characterized by high retraction rates, certain topics may only encounter minor issues, whereas in fields with lower retraction rates, some topics can experience substantial challenges. Moreover, an unexpected surge in publications has been observed in specific topics that also display abnormally high retraction rates. This study underscores several indicators that can assist the scientific community in pinpointing key fields that require rigorous scrutiny for potential low-quality and fraudulent research. Ultimately, our findings could serve as a benchmark for examining scientific integrity across diverse topics and offer crucial insights for developing tailored governance policies to enhance research integrity in each field."}
{"id": "2511.21117", "categories": ["nucl-th", "hep-ph"], "pdf": "https://arxiv.org/pdf/2511.21117", "abs": "https://arxiv.org/abs/2511.21117", "authors": ["Sheng-nan Han", "Jing Wu", "Yong-rui Chen", "Yi-zhen Huang", "Feng Li", "Wei-jie Fu"], "title": "Deuteron yields near the QCD phase transition", "comment": "10 pages, 9 figures", "summary": "We investigate the influence of QCD phase transition and critical fluctuations of the critical end point (CEP) on the deuteron yield within the functional renormalization group (fRG) approach, by using the nucleon coalescence model and a low energy effective field theory of quarks and mesons. It is found that the two-point baryon density correlation function is enhanced in a narrow region radiated from the CEP along the phase boundary. The deuteron yield arising from the two-point baryon correlation is small compared to the leading-order contribution, which is attributed to the fact that in the regime of low collision energy, i.e., the region of large baryon chemical potential, the freeze-out curves deviate from the critical region, resulting in that the enhancement of the deuteron yield stemming from the critical fluctuations near the CEP is mild."}
{"id": "2511.21239", "categories": ["nucl-ex"], "pdf": "https://arxiv.org/pdf/2511.21239", "abs": "https://arxiv.org/abs/2511.21239", "authors": ["Vendulka Humlova"], "title": "First measurement of the energy and Mandelstam-$t$ dependence of both coherent and incoherent $J/ψ$ photonuclear production", "comment": "7 pages, 3 figures. Proceedings of The second international workshop on the physics of Ultra Peripheral Collisions (UPC2025)", "summary": "A new phenomenon, gluon saturation, is expected to emerge in quantum chromodynamics (QCD) at high energies, when gluon splitting and recombination processes reach a dynamic equilibrium. In heavy nuclei, this balance is expected to be achieved at lower energies than in protons, making lead-lead collisions at the LHC an ideal environment to probe the onset of saturation. The diffractive photoproduction of the $J/ψ$ vector meson provides an excellent tool to study this regime since it directly probes the gluon distribution in the target. ALICE offers unique kinematic coverage of the photon-nucleon centre-of-mass energy, spanning from 20 to 800 GeV, corresponding to three orders of magnitude in Bjorken-$x$ from about $10^{-2}$ down to $10^{-5}$. This contribution presents the latest ALICE results on the energy dependence of coherent $J/ψ$ production, which is sensitive to the average gluon density, and on the energy and Mandelstam-$t$ dependence of incoherent production, which probes fluctuations of the gluon field at different spatial scales. These measurements provide unprecedented constraints on models of QCD in the high-energy limit and mark a milestone in studying the gluonic structure of nuclei."}
{"id": "2511.20874", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.20874", "abs": "https://arxiv.org/abs/2511.20874", "authors": ["Ashutossh Gupta", "Vassilis Kekatos", "Dionysios Aliprantis", "Steve Pekarek"], "title": "Dynamic Modeling of Load Demand in Electrified Highways Based on the EV Composition", "comment": "5 pages, 3 figures, 1 table", "summary": "Electrified roadways (ERs) equipped with the dynamic wireless power transfer (DWPT) technology can achieve longer driving range and reduce on-board battery requirements for electric vehicles (EVs). Due to the spatial arrangement of transmitter (Tx) coils embedded into the ER pavement, the power drawn by the EV's receiver (Rx) coil is oscillatory in nature. Therefore, understanding the dynamic behavior of the total DWPT load is important for power system dynamic studies. To this end, we model the load of individual EVs in the time and frequency domains for constant EV speed. We establish that a nonlinear control scheme implemented in existing DWPT-enabled EVs exhibits milder frequency harmonics compared to its linear alternative. According to this model, the harmonics of an EV load decrease in amplitude with the Rx coil length. We further propose and analyze stochastic models for the total DWPT load served by an ER segment. Our models explain how the EV composition on the ER affects its frequency spectrum. Interestingly, we show that serving more EVs with longer Rx coils (trucks) does not necessarily entail milder harmonics. Our analytical findings are corroborated using realistic flows from a traffic simulator and offer valuable insights to grid operators and ER designers."}
{"id": "2511.20904", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.20904", "abs": "https://arxiv.org/abs/2511.20904", "authors": ["Mengliang ZHang"], "title": "Generating Querying Code from Text for Multi-Modal Electronic Health Record", "comment": null, "summary": "Electronic health records (EHR) contain extensive structured and unstructured data, including tabular information and free-text clinical notes. Querying relevant patient information often requires complex database operations, increasing the workload for clinicians. However, complex table relationships and professional terminology in EHRs limit the query accuracy. In this work, we construct a publicly available dataset, TQGen, that integrates both \\textbf{T}ables and clinical \\textbf{T}ext for natural language-to-query \\textbf{Gen}eration. To address the challenges posed by complex medical terminology and diverse types of questions in EHRs, we propose TQGen-EHRQuery, a framework comprising a medical knowledge module and a questions template matching module. For processing medical text, we introduced the concept of a toolset, which encapsulates the text processing module as a callable tool, thereby improving processing efficiency and flexibility. We conducted extensive experiments to assess the effectiveness of our dataset and workflow, demonstrating their potential to enhance information querying in EHR systems."}
{"id": "2511.20834", "categories": ["cs.DC", "cs.AR", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.20834", "abs": "https://arxiv.org/abs/2511.20834", "authors": ["Dionysios Adamopoulos", "Anastasia Poulopoulou", "Georgios Goumas", "Christina Giannoula"], "title": "Accelerating Sparse Convolutions in Voxel-Based Point Cloud Networks", "comment": null, "summary": "Sparse Convolution (SpC) powers 3D point cloud networks widely used in autonomous driving and AR/VR. SpC builds a kernel map that stores mappings between input voxel coordinates, output coordinates, and weight offsets, then uses this map to compute feature vectors for output coordinates. Our work identifies three key properties of voxel coordinates: they are integer-valued, bounded within a limited spatial range, and geometrically continuous-neighboring voxels on the same object surface are highly likely to exist at small spatial offsets from each other. Prior SpC engines do not fully exploit these properties and suffer from high pre-processing and post-processing overheads during kernel map construction. To address this, we design Spira, the first voxel-property-aware SpC engine for GPUs. Spira proposes: (i) a high-performance one-shot search algorithm that builds the kernel map with no preprocessing and high memory locality, (ii) an effective packed-native processing scheme that accesses packed voxel coordinates at low cost, (iii) a flexible dual-dataflow execution mechanism that efficiently computes output feature vectors by adapting to layer characteristics, and (iv) a network-wide parallelization strategy that builds kernel maps for all SpC layers concurrently at network start. Our evaluation shows that Spira significantly outperforms prior SpC engines by 1.71x on average and up to 2.31x for end-to-end inference, and by 2.13x on average and up to 3.32x for layer-wise execution across diverse layer configurations."}
{"id": "2511.20955", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20955", "abs": "https://arxiv.org/abs/2511.20955", "authors": ["Sanchit Kaul", "Kevin Nhu", "Jason Eissayou", "Ivan Eser", "Victor Borup"], "title": "SpaceX: Exploring metrics with the SPACE model for developer productivity", "comment": "Code available at https://github.com/knhu/ECS260Project", "summary": "This empirical investigation elucidates the limitations of deterministic, unidimensional productivity heuristics by operationalizing the SPACE framework through extensive repository mining. Utilizing a dataset derived from open-source repositories, the study employs rigorous statistical methodologies including Generalized Linear Mixed Models (GLMM) and RoBERTa-based sentiment classification to synthesize a holistic, multi-faceted productivity metric. Analytical results reveal a statistically significant positive correlation between negative affective states and commit frequency, implying a cycle of iterative remediation driven by frustration. Furthermore, the investigation has demonstrated that analyzing the topology of contributor interactions yields superior fidelity in mapping collaborative dynamics compared to traditional volume-based metrics. Ultimately, this research posits a Composite Productivity Score (CPS) to address the heterogeneity of developer efficacy."}
{"id": "2511.21505", "categories": ["cs.DL", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2511.21505", "abs": "https://arxiv.org/abs/2511.21505", "authors": ["Sarah J. James", "Marcus A. Rodriguez", "David P. Miller"], "title": "The Intertwined Rise of Collaboration Scale, Reference Diversity, and Breakthrough Potential in Modern Science: A 40-Year Cross-Disciplinary Study", "comment": null, "summary": "Over the last four decades, the way knowledge is created in academia has transformed dramatically: research teams have grown larger, scholars draw from ever-wider pools of prior work, and the most influential discoveries increasingly emerge from complex collaborative efforts. Using a massive dataset of over 15 million publications spanning 1970-2010 and covering six major domains (Humanities, Social Sciences, Agricultural Sciences, Medical and Health Sciences, Engineering and Technology, and Natural Sciences), this study tracks how three core features of scientific papers - authorship team size, the breadth and variety of cited sources, and eventual citation impact - have co-evolved over time. We uncover striking differences across disciplines. In every field, papers that build on a broader and more diverse knowledge base consistently attract more citations later on, lending large-scale empirical support to theories that view scientific breakthroughs as outcomes of novel recombination across distant ideas. Bigger teams, on average, generate work with greater ultimate influence, but the gains taper off after a certain scale; very large consortia seldom produce the absolute highest-impact papers. While the Humanities and Social Sciences remain anchored in solo or small-group authorship traditions, the Natural Sciences, Medicine, and Engineering have moved decisively toward big-team mega-science. These patterns illuminate the underlying production technology of discovery, reveal discipline-specific barriers to collaboration and idea integration, and offer evidence-based guidance for research funding agencies, universities, and policymakers seeking to organize scientific work for maximum breakthrough potential."}
{"id": "2511.21141", "categories": ["nucl-th", "astro-ph.HE", "astro-ph.SR", "hep-ph"], "pdf": "https://arxiv.org/pdf/2511.21141", "abs": "https://arxiv.org/abs/2511.21141", "authors": ["Mannque Rho"], "title": "A Bottom-Up EFT Approach To Superdense Baryonic Matter", "comment": "6 pages, 2 figures", "summary": "How to arrive at the densest matter in massive compact stars starting from Walecka's linear $ω$-$σ$ mean-field model is described in a series of arguments anchored on hidden local symmetry, hidden scale symmetry and emergent parity-doublet symmetry. I follow the bottom-up approach from chiral symmetry with pions, coupled to hidden local and scale symmetry degrees of freedom. Exploiting the renormalization-group treatment à la Shankar and Polchinski of the fermionic interactions on the Fermi sphere, leading to Landau-Migdal Fermi-liquid, one obtains a sort of generalized ``Density Functional\" that allows via a topology change hadrons transform to quarks without phase changes at the center of massive stars. The highly dense matter is ``pseudo-conformal\" with the sound velocity $v_{pcs}^2/c^2\\approx 1/3$ but the trace of the energy-momentum tensor is not equal to zero, hence the matter is non-conformal."}
{"id": "2511.20895", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.20895", "abs": "https://arxiv.org/abs/2511.20895", "authors": ["Kimia Ahmadi", "Wouter A. Serdijn"], "title": "Adaptive Gradient Descent MPPT Algorithm With Complexity-Aware Benchmarking for Low-Power PV Systems", "comment": "12 pages, 13 figures", "summary": "This paper proposes a computationally efficient, real-time maximum power point tracking (MPPT) algorithm tailored for low-power photovoltaic (PV) systems operating under fast-changing irradiance and partial shading conditions (PSC). The proposed method augments the classical perturb and observe (P&O) algorithm with an adaptive gradient descent mechanism that dynamically scales the perturbation step size based on the instantaneous power-voltage slope, thereby minimizing tracking time and steady-state oscillations. An optional initialization routine enhances global MPP (GMPP) tracking under PSC. Extensive simulations, including irradiance recordings from freely moving rodent subjects relevant to the targeted application, and tests across varying converter topologies and temperatures, demonstrate its robust, topology-independent performance. The proposed algorithm achieves 99.94 percent MPPT efficiency under standard test conditions (STC), 99.21 percent when applied to experimental data, and more than 99.6 percent for the tested temperature profiles. Under PSC, the initialization routine improves tracking efficiency by up to 7.8 percent. A normalized gate-level complexity analysis and a unified figure-of-merit (FoM) incorporating efficiency, tracking time, and computational cost demonstrate that the proposed algorithm outperforms 35 state-of-the-art P&O-based MPPT algorithms. These results underscore its suitability for integration in low-power power management integrated circuits (PMICs) operating under dynamic and resource-constrained conditions."}
{"id": "2511.21121", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21121", "abs": "https://arxiv.org/abs/2511.21121", "authors": ["Anup Roy", "Rishabh Gyanendra Upadhyay", "Animesh Rameshbhai Panara", "Robin Mills"], "title": "Beyond Patch Aggregation: 3-Pass Pyramid Indexing for Vision-Enhanced Document Retrieval", "comment": null, "summary": "Document centric RAG pipelines usually begin with OCR, followed by brittle heuristics for chunking, table parsing, and layout reconstruction. These text first workflows are costly to maintain, sensitive to small layout shifts, and often lose the spatial cues that contain the answer. Vision first retrieval has emerged as a strong alternative. By operating directly on page images, systems like ColPali and ColQwen preserve structure and reduce pipeline complexity while achieving strong benchmark performance. However, these late interaction models tie retrieval to a specific vision backbone and require storing hundreds of patch embeddings per page, creating high memory overhead and complicating large scale deployment.\n  We introduce VisionRAG, a multimodal retrieval system that is OCR free and model agnostic. VisionRAG indexes documents directly as images, preserving layout, tables, and spatial cues, and builds semantic vectors without committing to a specific extraction. Our three pass pyramid indexing framework creates vectors using global page summaries, section headers, visual hotspots, and fact level cues. These summaries act as lightweight retrieval surrogates. At query time, VisionRAG retrieves the most relevant pages using the pyramid index, then forwards the raw page image encoded as base64 to a multimodal LLM for final question answering. During retrieval, reciprocal rank fusion integrates signals across the pyramid to produce robust ranking.\n  VisionRAG stores only 17 to 27 vectors per page, matching the efficiency of patch based methods while staying flexible across multimodal encoders. On financial document benchmarks, it achieves 0.8051 accuracy at 10 on FinanceBench and 0.9629 recall at 100 on TAT DQA. These results show that OCR free, summary guided multimodal retrieval is a practical and scalable alternative to traditional text extraction pipelines."}
{"id": "2511.20975", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.20975", "abs": "https://arxiv.org/abs/2511.20975", "authors": ["Yinwei Dai", "Zhuofu Chen", "Anand Iyer", "Ravi Netravali"], "title": "Aragog: Just-in-Time Model Routing for Scalable Serving of Agentic Workflows", "comment": null, "summary": "Agentic workflows have emerged as a powerful paradigm for solving complex, multi-stage tasks, but serving them at scale is computationally expensive given the many LLM inferences that each request must pass through. Configuration selection, or the cost-aware assignment of workflow agents to specific LLMs, can reduce these costs, but existing approaches bind configuration decisions before request execution, making them ill-suited for the heterogeneous and lengthy execution of workflows. Specifically, system loads can fluctuate rapidly and substantially during a request's lifetime, causing fixed configurations to quickly become suboptimal. We present Aragog, a system that progressively adapts a request's configuration throughout its execution to match runtime dynamics. To make this practical despite the massive space of workflow configurations, Aragog decouples the problem into two core elements -- a one-time routing step that identifies all accuracy-preserving configurations, and a cheap per-stage scheduler that selects among them using up-to-date system observations -- and introduces novel strategies to accelerate each. Across diverse workflows and model families, Aragog increases maximum serving throughput by 50.0--217.0\\% and reduces median latency by 32.5--78.9\\% at peak request rates, while maintaining accuracy comparable to the most expensive configurations."}
{"id": "2511.21022", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21022", "abs": "https://arxiv.org/abs/2511.21022", "authors": ["Guancheng Lin", "Xiao Yu", "Jacky Keung", "Xing Hu", "Xin Xia", "Alex X. Liu"], "title": "Lightweight Model Editing for LLMs to Correct Deprecated API Recommendations", "comment": null, "summary": "Pre-trained or fine-tuned on large code corpora, Large Language Models (LLMs) have demonstrated strong performance in code completion tasks. However, their embedded knowledge is constrained by the timeliness of training data, which often includes code using deprecated APIs. Consequently, LLMs frequently generate deprecated APIs that will no longer be supported in future versions of third-party libraries. While retraining LLMs on updated codebases could refresh their API knowledge, this approach is computationally expensive. Recently, lightweight model editing methods have emerged to efficiently correct specific knowledge in LLMs. However, it remains unclear whether these methods can effectively update deprecated API knowledge and enable edited models to generate up-to-date APIs. To address this gap, we conduct the first systematic study applying 10 state-of-the-art model editing techniques to update deprecated API knowledge in three LLMs: Qwen2.5-Coder, StarCoder2, and DeepSeek-Coder. We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances. Our results show that the parameter-efficient fine-tuning method AdaLoRA achieves the best performance in enabling edited models to generate correct, up-to-date APIs, but falls short in Specificity (i.e., the editing influences untargeted knowledge). To resolve this, we propose AdaLoRA-L, which defines \"Common API Layers\" (layers within the LLMs with high importance across all APIs, storing general knowledge and excluded from editing) and restricts edits exclusively to \"Specific API Layers\" (layers with high importance only for the target API, storing the API-specific knowledge). Experimental results demonstrate that AdaLoRA-L significantly improves Specificity while maintaining comparable performance across other evaluation metrics."}
{"id": "2511.20914", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.20914", "abs": "https://arxiv.org/abs/2511.20914", "authors": ["Vivek Pandey", "Nader Motee"], "title": "Distributionally Robust Cascading Risk in Multi-Agent Rendezvous: Extended Analysis of Parameter-Induced Ambiguity", "comment": null, "summary": "Ensuring safety in autonomous multi-agent systems during time-critical tasks such as rendezvous is a fundamental challenge, particularly under communication delays and uncertainty in system parameters. In this paper, we develop a theoretical framework to analyze the \\emph{distributionally robust risk of cascading failures} in multi-agent rendezvous, where system parameters lie within bounded uncertainty sets around nominal values. Using a time-delayed dynamical network as a benchmark model, we quantify how small deviations in these parameters impact collective safety. We introduce a \\emph{conditional distributionally robust functional}, grounded in a bivariate Gaussian model, to characterize risk propagation between agents. This yields a \\emph{closed-form risk expression} that captures the complex interaction between time delays, network structure, noise statistics, and failure modes. These expressions expose key sensitivity patterns and provide actionable insight for the design of robust and resilient multi-agent networks. Extensive simulations validate the theoretical results and demonstrate the effectiveness of our framework."}
{"id": "2511.21389", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21389", "abs": "https://arxiv.org/abs/2511.21389", "authors": ["Guoxiao Zhang", "Ao Li", "Tan Qu", "Qianlong Xie", "Xingxing Wang"], "title": "FITRep: Attention-Guided Item Representation via MLLMs", "comment": null, "summary": "Online platforms usually suffer from user experience degradation due to near-duplicate items with similar visuals and text. While Multimodal Large Language Models (MLLMs) enable multimodal embedding, existing methods treat representations as black boxes, ignoring structural relationships (e.g., primary vs. auxiliary elements), leading to local structural collapse problem. To address this, inspired by Feature Integration Theory (FIT), we propose FITRep, the first attention-guided, white-box item representation framework for fine-grained item deduplication. FITRep consists of: (1) Concept Hierarchical Information Extraction (CHIE), using MLLMs to extract hierarchical semantic concepts; (2) Structure-Preserving Dimensionality Reduction (SPDR), an adaptive UMAP-based method for efficient information compression; and (3) FAISS-Based Clustering (FBC), a FAISS-based clustering that assigns each item a unique cluster id using FAISS. Deployed on Meituan's advertising system, FITRep achieves +3.60% CTR and +4.25% CPM gains in online A/B tests, demonstrating both effectiveness and real-world impact."}
{"id": "2511.20982", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.20982", "abs": "https://arxiv.org/abs/2511.20982", "authors": ["Junhan Liao", "Minxian Xu", "Wanyi Zheng", "Yan Wang", "Kejiang Ye", "Rajkumar Buyya", "Chengzhong Xu"], "title": "A Dynamic PD-Disaggregation Architecture for Maximizing Goodput in LLM Inference Serving", "comment": "14 pages", "summary": "To meet strict Service-Level Objectives (SLOs),contemporary Large Language Models (LLMs) decouple the prefill and decoding stages and place them on separate GPUs to mitigate the distinct bottlenecks inherent to each phase. However, the heterogeneity of LLM workloads causes producerconsumer imbalance between the two instance types in such disaggregated architecture. To address this problem, we propose DOPD (Dynamic Optimal Prefill/Decoding), a dynamic LLM inference system that adjusts instance allocations to achieve an optimal prefill-to-decoding (P/D) ratio based on real-time load monitoring. Combined with an appropriate request-scheduling policy, DOPD effectively resolves imbalances between prefill and decoding instances and mitigates resource allocation mismatches due to mixed-length requests under high concurrency. Experimental evaluations show that, compared with vLLM and DistServe (representative aggregation-based and disaggregationbased approaches), DOPD improves overall system goodput by up to 1.5X, decreases P90 time-to-first-token (TTFT) by up to 67.5%, and decreases P90 time-per-output-token (TPOT) by up to 22.8%. Furthermore, our dynamic P/D adjustment technique performs proactive reconfiguration based on historical load, achieving over 99% SLOs attainment while using less additional resources."}
{"id": "2511.21151", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21151", "abs": "https://arxiv.org/abs/2511.21151", "authors": ["M. Alecci", "P. Jiménez", "J. Samhi", "T. Bissyandé", "J. Klein"], "title": "Exploring Hidden Geographic Disparities in Android Apps", "comment": null, "summary": "While mobile app evolution has been widely studied, geographical variation in app behavior remains largely unexplored. This paper presents a large-scale study of location-based Android app differentiation, uncovering two important and underexamined phenomena with security and fairness implications. First, we introduce GeoTwins: apps that are functionally similar and share branding but are released under different package names across countries. Despite their similarity, GeoTwins often diverge in requested permissions, third-party libraries, and privacy disclosures. Second, we examine the Android App Bundle ecosystem and reveal unexpected regional differences in supposedly consistent base.apk files. Contrary to common assumptions, even base.apk files vary by region, exposing hidden customizations that may affect app behavior or security.\n  These discrepancies have concrete consequences. Geographically distinct variants can lead the same app to be labeled benign in one malware study but suspicious in another, depending on the region of download. Such hidden variation undermines reproducibility and introduces geographic bias into assessments of security, privacy, and functionality. It also raises ethical concerns about transparency and consent: visually identical Google Play listings may mask subtle but important differences.\n  To study these issues, we built a distributed app collection pipeline spanning multiple regions and analyzed thousands of apps. We also release a dataset of 81,963 GeoTwins to support future work. Our findings reveal systemic regional disparities in mobile software, with implications for researchers, developers, platform architects, and policymakers."}
{"id": "2511.20939", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.20939", "abs": "https://arxiv.org/abs/2511.20939", "authors": ["Youhong Chen", "Debraj Bhattacharjee", "Balarko Chaudhuri", "Mark O Malley", "Nan Qin", "Adrian Pilkaer Expethit"], "title": "Data-Driven Post-Event Analysis with Real-World Oscillation Data from Denmark", "comment": "5 pages, 6 figures, real power network event data, submitted to IEEE General Meeting 2026", "summary": "This paper demonstrates how Extended Dynamic Mode Decomposition (EDMD), grounded in Koopman operator theory, can effectively identify the main contributor(s) to oscillations in power grids. We use PMU data recorded from a real 0.15 Hz oscillation event in Denmark for post-event analysis. To this end, the EDMD algorithm processed only voltage and current phasors from nineteen PMUs at different voltage levels across the Danish grid. In such a blind-test setting with no supplementary system information, EDMD accurately pinpointed the location of the main contributor to the 0.2 Hz oscillation, consistent with the location of the problematic IBR plant later confirmed by Energinet, where the underlying cause was a control system issue. Conventional approaches, such as the dissipating energy flow (DEF) method used in the ISO-NE OSL tool did not clearly identify this plant. This joint validation with Energinet, reinforcing earlier studies using simulated IBR-dominated systems and real PMU data from ISO-NE, highlights the potential of EDMD-based post-event analysis for identifying major oscillation contributors and enabling targeted SSO mitigation."}
{"id": "2511.21394", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21394", "abs": "https://arxiv.org/abs/2511.21394", "authors": ["Guoxiao Zhang", "Tan Qu", "Ao Li", "DongLin Ni", "Qianlong Xie", "Xingxing Wang"], "title": "RIA: A Ranking-Infused Approach for Optimized listwise CTR Prediction", "comment": null, "summary": "Reranking improves recommendation quality by modeling item interactions. However, existing methods often decouple ranking and reranking, leading to weak listwise evaluation models that suffer from combinatorial sparsity and limited representational power under strict latency constraints. In this paper, we propose RIA (Ranking-Infused Architecture), a unified, end-to-end framework that seamlessly integrates pointwise and listwise evaluation. RIA introduces four key components: (1) the User and Candidate DualTransformer (UCDT) for fine-grained user-item-context modeling; (2) the Context-aware User History and Target (CUHT) module for position-sensitive preference learning; (3) the Listwise Multi-HSTU (LMH) module to capture hierarchical item dependencies; and (4) the Embedding Cache (EC) module to bridge efficiency and effectiveness during inference. By sharing representations across ranking and reranking, RIA enables rich contextual knowledge transfer while maintaining low latency. Extensive experiments show that RIA outperforms state-of-the-art models on both public and industrial datasets, achieving significant gains in AUC and LogLoss. Deployed in Meituan advertising system, RIA yields a +1.69% improvement in Click-Through Rate (CTR) and a +4.54% increase in Cost Per Mille (CPM) in online A/B tests."}
{"id": "2511.21018", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.21018", "abs": "https://arxiv.org/abs/2511.21018", "authors": ["Antonis Psistakis"], "title": "Handling of Memory Page Faults during Virtual-Address RDMA", "comment": "Antonis Psistakis, Master of Science (MSc) Thesis 2019. The abstract and text were lightly revised in 2025 to comply with arXiv formatting guidelines", "summary": "Nowadays, avoiding system calls during cluster communication (e.g., in Data Centers and High Performance Computing) in modern high-speed interconnection networks has become a necessity, due to the high overhead of multiple data copies between kernel and user space. User-level zero-copy Remote Direct Memory Access (RDMA) technologies address this problem by improving performance and reducing system energy consumption. However, traditional RDMA engines cannot tolerate page faults and therefore use various techniques to avoid them.\n  State-of-the-art RDMA approaches typically rely on pinning address spaces or multiple pages per application. This method introduces long-term disadvantages due to increased programming complexity (pinning and unpinning buffers), limits on how much memory can be pinned, and inefficient memory utilization. In addition, pinning does not fully prevent page faults because modern operating systems apply internal optimization mechanisms, such as Transparent Huge Pages (THP), which are enabled by default in Linux.\n  This thesis implements a page-fault handling mechanism integrated with the DMA engine of the ExaNeSt project. Faults are detected by the ARM System Memory Management Unit (SMMU) and resolved through a hardware-software solution that can request retransmission when needed. This mechanism required modifications to the Linux SMMU driver, the development of a new software library, changes to the DMA engine hardware, and adjustments to the DMA scheduling logic. Experiments were conducted on the Quad-FPGA Daughter Board (QFDB) of ExaNeSt, which uses Xilinx Zynq UltraScale+ MPSoCs.\n  Finally, we evaluate our mechanism and compare it against alternatives such as pinning and pre-faulting, and discuss the advantages of our approach."}
{"id": "2511.21197", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.21197", "abs": "https://arxiv.org/abs/2511.21197", "authors": ["Paolo Buono", "Mary Cerullo", "Stefano Cirillo", "Giuseppe Desolda", "Francesco Greco", "Emanuela Guglielmi", "Grazia Margarella", "Giuseppe Polese", "Simone Scalabrino", "Cesare Tucci"], "title": "Bug Detective and Quality Coach: Developers' Mental Models of AI-Assisted IDE Tools", "comment": null, "summary": "AI-assisted tools support developers in performing cognitively demanding tasks such as bug detection and code readability assessment. Despite the advancements in the technical characteristics of these tools, little is known about how developers mentally model them and how mismatches affect trust, control, and adoption. We conducted six co-design workshops with 58 developers to elicit their mental models about AI-assisted bug detection and readability features. It emerged that developers conceive bug detection tools as \\textit{bug detectives}, which warn users only in case of critical issues, guaranteeing transparency, actionable feedback, and confidence cues. Readability assessment tools, on the other hand, are envisioned as \\textit{quality coaches}, which provide contextual, personalized, and progressive guidance. Trust, in both tasks, depends on the clarity of explanations, timing, and user control. A set of design principles for Human-Centered AI in IDEs has been distilled, aiming to balance disruption with support, conciseness with depth, and automation with human agency."}
{"id": "2511.20977", "categories": ["eess.SY", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.20977", "abs": "https://arxiv.org/abs/2511.20977", "authors": ["Junkai Hu", "Li Xia"], "title": "Independent policy gradient-based reinforcement learning for economic and reliable energy management of multi-microgrid systems", "comment": null, "summary": "Efficiency and reliability are both crucial for energy management, especially in multi-microgrid systems (MMSs) integrating intermittent and distributed renewable energy sources. This study investigates an economic and reliable energy management problem in MMSs under a distributed scheme, where each microgrid independently updates its energy management policy in a decentralized manner to optimize the long-term system performance collaboratively. We introduce the mean and variance of the exchange power between the MMS and the main grid as indicators for the economic performance and reliability of the system. Accordingly, we formulate the energy management problem as a mean-variance team stochastic game (MV-TSG), where conventional methods based on the maximization of expected cumulative rewards are unsuitable for variance metrics. To solve MV-TSGs, we propose a fully distributed independent policy gradient algorithm, with rigorous convergence analysis, for scenarios with known model parameters. For large-scale scenarios with unknown model parameters, we further develop a deep reinforcement learning algorithm based on independent policy gradients, enabling data-driven policy optimization. Numerical experiments in two scenarios validate the effectiveness of the proposed methods. Our approaches fully leverage the distributed computational capabilities of MMSs and achieve a well-balanced trade-off between economic performance and operational reliability."}
{"id": "2511.21413", "categories": ["cs.DC", "cs.AI", "cs.DB", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.21413", "abs": "https://arxiv.org/abs/2511.21413", "authors": ["Tim Trappen", "Robert Keßler", "Roland Pabel", "Viktor Achter", "Stefan Wesner"], "title": "Automated Dynamic AI Inference Scaling on HPC-Infrastructure: Integrating Kubernetes, Slurm and vLLM", "comment": "6 pages, 3 figures", "summary": "Due to rising demands for Artificial Inteligence (AI) inference, especially in higher education, novel solutions utilising existing infrastructure are emerging. The utilisation of High-Performance Computing (HPC) has become a prevalent approach for the implementation of such solutions. However, the classical operating model of HPC does not adapt well to the requirements of synchronous, user-facing dynamic AI application workloads. In this paper, we propose our solution that serves LLMs by integrating vLLM, Slurm and Kubernetes on the supercomputer \\textit{RAMSES}. The initial benchmark indicates that the proposed architecture scales efficiently for 100, 500 and 1000 concurrent requests, incurring only an overhead of approximately 500 ms in terms of end-to-end latency."}
{"id": "2511.21380", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21380", "abs": "https://arxiv.org/abs/2511.21380", "authors": ["Jingyi Chen", "Xiaoyan Guo", "Songqiang Chen", "Shing-Chi Cheung", "Jiasi Shen"], "title": "Multi-Agent Systems for Dataset Adaptation in Software Engineering: Capabilities, Limitations, and Future Directions", "comment": null, "summary": "Automating the adaptation of software engineering (SE) research artifacts across datasets is essential for scalability and reproducibility, yet it remains largely unstudied. Recent advances in large language model (LLM)-based multi-agent systems, such as GitHub Copilot's agent mode, promise to automate complex development workflows through coordinated reasoning, code generation, and tool interaction. This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0. Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance. Results show that current systems can identify key files and generate partial adaptations but rarely produce functionally correct implementations. Prompt-level interventions, especially providing execution error messages and reference code, substantially improve structural similarity to ground truth (from 7.25% to 67.14%), highlighting the importance of contextual and feedback-driven guidance. Our findings reveal both the promise and limitations of today's multi-agent LLM systems for dataset adaptation, and suggest concrete directions for building more reliable, self-correcting agents in future SE research."}
{"id": "2511.20995", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.20995", "abs": "https://arxiv.org/abs/2511.20995", "authors": ["Felix Biertümpfel", "Bin Hu", "Geir Dullerud", "Peter Seiler"], "title": "An Exact, Finite Dimensional Representation for Full-Block, Circle Criterion Multipliers", "comment": null, "summary": "This paper provides the first finite-dimensional characterization for the complete set of full-block, circle criterion multipliers. We consider the interconnection of a discrete-time, linear time-invariant system in feedback with a non-repeated, sector-bounded nonlinearity. Sufficient conditions for stability and performance can be derived using: (i) dissipation inequalities, and (ii) Quadratic Constraints (QCs) that bound the input/output pairs of the nonlinearity. Larger classes of QCs (or multipliers) reduce the conservatism of the conditions. Full-block, circle criterion multipliers define the complete set of all possible QCs for non-repeated, sector-bounded nonlinearities. These provide the least conservative conditions. However, full-block multipliers are defined by an uncountably infinite number of constraints and hence do not lead to computationally tractable solutions if left in this raw form. This paper provides a new finite-dimensional characterization for the set of full-block, circle criterion multipliers. The key theoretical insight is: the set of all input/output pairs of non-repeated sector-bounded nonlinearities is equal to the set of all incremental pairs for an appropriately constructed piecewise linear function. Our new description for the complete set of multipliers only requires a finite number of matrix copositivity constraints. These conditions have an exact, computationally tractable implementation for problems where the nonlinearity has small input/output dimensions $(\\le 4)$. We illustrate the use of our new characterization via a simple example."}
{"id": "2511.21431", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.21431", "abs": "https://arxiv.org/abs/2511.21431", "authors": ["Lu Zhao", "Rong Shi", "Shaoqing Zhang", "Yueqiang Chen", "Baoguo He", "Hongfeng Sun", "Ziqing Yin", "Shangchao Su", "Zhiyan Cui", "Liang Dong", "Xiyuan Li", "Lingbin Wang", "Jianwei He", "Jiesong Ma", "Weikang Huang", "Jianglei Tong", "Dongdong Gao", "Jian Zhang", "Hong Tian"], "title": "MemFine: Memory-Aware Fine-Grained Scheduling for MoE Training", "comment": null, "summary": "The training of large-scale Mixture of Experts (MoE) models faces a critical memory bottleneck due to severe load imbalance caused by dynamic token routing. This imbalance leads to memory overflow on GPUs with limited capacity, constraining model scalability. Existing load balancing methods, which cap expert capacity, compromise model accuracy and fail on memory-constrained hardware. To address this, we propose MemFine, a memory-aware fine-grained scheduling framework for MoE training. MemFine decomposes the token distribution and expert computation into manageable chunks and employs a chunked recomputation strategy, dynamically optimized through a theoretical memory model to balance memory efficiency and throughput. Experiments demonstrate that MemFine reduces activation memory by 48.03% and improves throughput by 4.42% compared to full recomputation-based baselines, enabling stable large-scale MoE training on memory-limited GPUs."}
{"id": "2511.21382", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21382", "abs": "https://arxiv.org/abs/2511.21382", "authors": ["Bei Chu", "Yang Feng", "Kui Liu", "Zifan Nan", "Zhaoqiang Guo", "Baowen Xu"], "title": "Large Language Models for Unit Test Generation: Achievements, Challenges, and the Road Ahead", "comment": "33 pages, 8 figures", "summary": "Unit testing is an essential yet laborious technique for verifying software and mitigating regression risks. Although classic automated methods effectively explore program structures, they often lack the semantic information required to produce realistic inputs and assertions. Large Language Models (LLMs) address this limitation by utilizing by leveraging their data-driven knowledge of code semantics and programming patterns. To analyze the state of the art in this domain, we conducted a systematic literature review of 115 publications published between May 2021 and August 2025. We propose a unified taxonomy based on the unit test generation lifecycle that treats LLMs as stochastic generators requiring systematic engineering constraints. This framework analyzes the literature regarding core generative strategies and a set of enhancement techniques ranging from pre-generation context enrichment to post-generation quality assurance. Our analysis reveals that prompt engineering has emerged as the dominant utilization strategy and accounts for 89% of the studies due to its flexibility. We find that iterative validation and repair loops have become the standard mechanism to ensure robust usability and lead to significant improvements in compilation and execution pass rates. However, critical challenges remain regarding the weak fault detection capabilities of generated tests and the lack of standardized evaluation benchmarks. We conclude with a roadmap for future research that emphasizes the progression towards autonomous testing agents and hybrid systems combining LLMs with traditional software engineering tools. This survey provides researchers and practitioners with a comprehensive perspective on converting the potential of LLMs into industrial-grade testing solutions."}
{"id": "2511.21228", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.21228", "abs": "https://arxiv.org/abs/2511.21228", "authors": ["Anthony Couthures", "Gustave Bainier", "Vineeth Satheeskumar Varma", "Samson Lasaulce", "Irinel-Constantin Morarescu"], "title": "From Consensus to Robust Clustering: Multi-Agent Systems with Nonlinear Interactions", "comment": null, "summary": "This paper establishes a theoretical framework to describe the transition from consensus to stable clustering in multi-agent systems with nonlinear, cooperative interactions. We first establish a sharp threshold for consensus. For a broad class of non-decreasing, Lipschitz-continuous interactions, an explicit inequality linking the interaction's Lipschitz constant to the second-largest eigenvalue of the normalized adjacency matrix of the interaction graph confines all system equilibria to the synchronization manifold. This condition is shown to be a sharp threshold, as its violation permits the emergence of non-synchronized equilibria. We also demonstrate that such clustered states can only arise if the interaction law itself possesses specific structural properties, such as unstable fixed points. For the clustered states that emerge, we introduce a formal framework using Input-to-State Stability (ISS) theory to quantify their robustness. This approach allows us to prove that the internal cohesion of a cluster is robust to perturbations from the rest of the network. The analysis reveals a fundamental principle: cluster coherence is limited not by the magnitude of external influence, but by its heterogeneity across internal nodes. This unified framework, explaining both the sharp breakdown of consensus and the quantifiable robustness of the resulting modular structures, is validated on Zachary's Karate Club network, used as a classic benchmark for community structure."}
{"id": "2511.21535", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.21535", "abs": "https://arxiv.org/abs/2511.21535", "authors": ["Morteza Sadeghi"], "title": "Modeling the Effect of Data Redundancy on Speedup in MLFMA Near-Field Computation", "comment": null, "summary": "The near-field (P2P) operator in the Multilevel Fast Multipole Algorithm (MLFMA) is a performance bottleneck on GPUs due to poor memory locality. This work introduces data redundancy to improve spatial locality by reducing memory access dispersion. For validation of results, we propose an analytical model based on a Locality metric that combines data volume and access dispersion to predict speedup trends without hardware-specific profiling. The approach is validated on two MLFMA-based applications: an electromagnetic solver (DBIM-MLFMA) with regular structure, and a stellar dynamics code (PhotoNs-2.0) with irregular particle distribution. Results show up to 7X kernel speedup due to improved cache behavior. However, increased data volume raises overheads in data restructuring, limiting end-to-end application speedup to 1.04X. While the model cannot precisely predict absolute speedups, it reliably captures performance trends across different problem sizes and densities. The technique is injectable into existing implementations with minimal code changes. This work demonstrates that data redundancy can enhance GPU performance for P2P operator, provided locality gains outweigh data movement costs."}
{"id": "2511.21248", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.21248", "abs": "https://arxiv.org/abs/2511.21248", "authors": ["Irene Schimperna", "Lea Bold", "Johannes Köhler", "Karl Worthmann", "Lalo Magni"], "title": "Stability of data-driven Koopman MPC with terminal conditions", "comment": "8 pages, 1 figure", "summary": "This paper derives conditions under which Model Predictive Control (MPC) with terminal conditions, using a data-driven surrogate model as a prediction model, asymptotically stabilizes the plant despite approximation errors. In particular, we prove recursive feasibility and asymptotic stability if a proportional error bound holds, where proportional means that the bound is linear in the norm of the state and the input. For a broad class of nonlinear systems, this condition can be satisfied using data-driven surrogate models generated by kernel Extended Dynamic Mode Decomposition (kEDMD) using the Koopman operator. Last, the applicability of the proposed framework is demonstrated in a numerical case study."}
{"id": "2511.21612", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.21612", "abs": "https://arxiv.org/abs/2511.21612", "authors": ["Shahir Abdullah", "Syed Rohit Zaman"], "title": "Diagonal Scaling: A Multi-Dimensional Resource Model and Optimization Framework for Distributed Databases", "comment": null, "summary": "Modern cloud databases present scaling as a binary decision: scale-out by adding nodes or scale-up by increasing per-node resources. This one-dimensional view is limiting because database performance, cost, and coordination overhead emerge from the joint interaction of horizontal elasticity and per-node CPU, memory, network bandwidth, and storage IOPS. As a result, systems often overreact to load spikes, underreact to memory pressure, or oscillate between suboptimal states. We introduce the Scaling Plane, a two-dimensional model in which each distributed database configuration is represented as a point (H, V), with H denoting node count and V a vector of resources. Over this plane, we define smooth approximations of latency, throughput, coordination overhead, and monetary cost, providing a unified view of performance trade-offs. We show analytically and empirically that optimal scaling trajectories frequently lie along diagonal paths: sequences of joint horizontal and vertical adjustments that simultaneously exploit cluster parallelism and per-node improvements. To compute such actions, we propose DIAGONALSCALE, a discrete local-search algorithm that evaluates horizontal, vertical, and diagonal moves in the Scaling Plane and selects the configuration minimizing a multi-objective function subject to SLA constraints. Using synthetic surfaces, microbenchmarks, and experiments on distributed SQL and KV systems, we demonstrate that diagonal scaling reduces p95 latency by up to 40 percent, lowers cost-per-query by up to 37 percent, and reduces rebalancing by 2 to 5 times compared to horizontal-only and vertical-only autoscaling. Our results highlight the need for multi-dimensional scaling models and provide a foundation for next-generation autoscaling in cloud database systems."}
{"id": "2511.21255", "categories": ["eess.SY", "cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.21255", "abs": "https://arxiv.org/abs/2511.21255", "authors": ["Jewel Benny", "Pranjal Mahajan", "Srayan Sankar Chatterjee", "Mohd Wajid", "Abhishek Srivastava"], "title": "Design and Measurements of mmWave FMCW Radar Based Non-Contact Multi-Patient Heart Rate and Breath Rate Monitoring System", "comment": "Presented at BioCAS 2023", "summary": "Recent developments in mmWave radar technologies have enabled the truly non-contact heart-rate (HR) and breath-rate (BR) measurement approaches, which provides a great ease in patient monitoring. Additionally, these technologies also provide opportunities to simultaneously detect HR and BR of multiple patients, which has become increasingly important for efficient mass monitoring scenarios. In this work, a frequency modulated continuous wave (FMCW) mmWave radar based truly non-contact multiple patient HR and BR monitoring system has been presented. Furthermore, a novel approach is also proposed, which combines multiple processing methods using a least squares solution to improve measurement accuracy, generalization, and handle measurement error. The proposed system has been developed using Texas Instruments' FMCW radar and experimental results with multiple subjects are also presented, which show >97% and >93% accuracy in the measured BR and HR values, respectively."}
{"id": "2511.21661", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.21661", "abs": "https://arxiv.org/abs/2511.21661", "authors": ["Beth Plale", "Neelesh Karthikeyan", "Isuru Gamage", "Joe Stubbs", "Sachith Withana"], "title": "AI/ML Model Cards in Edge AI Cyberinfrastructure: towards Agentic AI", "comment": null, "summary": "AI/ML model cards can contain a benchmarked evaluation of an AI/ML model against intended use but a one time assessment during model training does not get at how and where a model is actually used over its lifetime. Through Patra Model Cards embedded in the ICICLE AI Institute software ecosystem we study model cards as dynamic objects. The study reported here assesses the benefits and tradeoffs of adopting the Model Context Protocol (MCP) as an interface to the Patra Model Card server. Quantitative assessment shows the overhead of MCP as compared to a REST interface. The core question however is of active sessions enabled by MCP; this is a qualitative question of fit and use in the context of dynamic model cards that we address as well."}
{"id": "2511.21269", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.21269", "abs": "https://arxiv.org/abs/2511.21269", "authors": ["Jinhui Chen", "Huadong Sun", "Ping Wu", "Baocai Wang", "Bing Zhao"], "title": "Response-Based Frequency Stability Assessment under Multi-Scale Disturbances in High-Renewable Power Systems", "comment": "10 pages, 13 figures", "summary": "In high-renewable power systems, active-power disturbances are becoming larger and exhibit increasingly diverse time scales, which complicates frequency stability assessment under unanticipated events. This paper presents a response-based frequency stability assessment method that uses disturbance power, inferred from generator electrical responses, to provide a unified treatment of multi-scale disturbances. Unanticipated disturbances are first classified into short-term and permanent events; permanent disturbances are further divided into step, second-level slope and minute-level slope disturbances. Based on the measured power responses of generator groups, a unified disturbance-power model is constructed to identify the disturbance type online and to quantify disturbance intensity through the disturbance power and its rate of change. Analytical frequency-response models are then derived for each disturbance class. For step disturbances, the maximum tolerable disturbance power is obtained under steady-state and transient frequency deviation constraints, and a safety-margin index is defined. For slope-type disturbances, an improved system frequency response (SFR) model and the rotor motion equation after exhaustion of primary frequency regulation are used to compute the over-limit time of frequency deviation. The proposed response-based assessment method is validated on the CSEE-FS frequency-stability benchmark system, demonstrating its effectiveness and accuracy for quantitative frequency stability assessment in high-renewable power systems."}
{"id": "2511.21271", "categories": ["eess.SY", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.21271", "abs": "https://arxiv.org/abs/2511.21271", "authors": ["Xinyan Xie", "Xuesong Wang", "Xin Lai", "Yongheng Wen", "Fengrui Yang", "Haoyang He", "Lai Zhang", "Dong Zhao"], "title": "Adaptive Lighting Control in Visible Light Systems: An Integrated Sensing, Communication, and Illumination Framework", "comment": null, "summary": "Indoor visible light communication (VLC) is a promising sixth-generation (6G) technology, as its directional and sensitive optical signals are naturally suited for integrated sensing and communication (ISAC). However, current research mainly focuses on maximizing data rates and sensing accuracy, creating a conflict between high performance, high energy consumption, and user visual comfort. This paper proposes an adaptive integrated sensing, communication, and illumination (ISCI) framework that resolves this conflict by treating energy savings as a primary objective. The framework's mechanism first partitions the receiving plane using a geometric methodology, defining an activity area and a surrounding non-activity area to match distinct user requirements. User location, determined using non-line-of-sight (NLOS) sensing, then acts as a dynamic switch for the system's optimization objective. The system adaptively shifts between minimizing total transmit power while guaranteeing communication and illumination performance in the activity area and maximizing signal-to-noise ratio (SNR) uniformity in the non-activity area. Numerical results confirm that this adaptive ISCI approach achieves 53.59% energy savings over a non-adaptive system and improves SNR uniformity by 57.79%, while satisfying all illumination constraints and maintaining a mean localization error of 0.071 m."}
{"id": "2511.21273", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.21273", "abs": "https://arxiv.org/abs/2511.21273", "authors": ["Ana Cordon-Avila", "Mostafa Selim", "Momen Abayazid"], "title": "Respiratory Motion Compensation and Haptic Feedback for X-ray-Guided Teleoperated Robotic Needle Insertion", "comment": null, "summary": "Respiratory motion limits the accuracy and precision of abdominal percutaneous procedures. In this paper, respiratory motion is compensated robotically using motion estimation models. Additionally, a teleoperated insertion is performed using proximity-based haptic feedback to guide physicians during insertion, enabling a radiation-free remote insertion for the end-user. The study has been validated using a robotic liver phantom, and five insertions were performed. The resulting motion estimation errors were below 3 mm for all directions of motion, and the overall resulting 3D insertion errors were 2.60, 7.75, and 2.86 mm for the superior-inferior, lateral, and anterior-posterior directions of motion, respectively. The proposed approach is expected to minimize the chances of inaccurate treatment or diagnosis due to respiratory-induced motion and reduce radiation exposure."}
{"id": "2511.21300", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.21300", "abs": "https://arxiv.org/abs/2511.21300", "authors": ["A. J. Alves Junior", "M. J. B. B. Davi", "R. A. S. Fernandes", "M. Oleskovicz", "D. V. Coury"], "title": "Data-Driven Reduction of Fault Location Errors in Onshore Wind Farm Collectors", "comment": null, "summary": "Accurate fault location is essential for operational reliability and fast restoration in wind farm collector networks. However, the growing integration of inverter-based resources changes the current and voltage behavior during faults, challenging the effectiveness of traditional phasor-based diagnostic methods. In this context, the present paper introduces an advanced machine-learning solution that enhances a deterministic fault distance estimator by incorporating a correction model driven by a Gated Residual Network, specifically designed to minimize residual fault location errors. Through comprehensive feature engineering and selection processes, an improved predictor was developed and trained on a diverse set of fault scenarios simulated in a PSCAD-based real-world wind farm model, including variations in fault type, resistance, location, inception angle, and generation penetration. Hyperparameter optimization was performed using the Optuna framework, and the robustness of the method was statistically validated. Results show a significant improvement in accuracy, with a 76% overall decrease in fault location error compared to state-of-the-art approaches. The proposed method demonstrates strong scalability and adaptability to topological and operational changes. This approach advances the deployment of data-driven fault location frameworks for modern power systems."}
{"id": "2511.21304", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.21304", "abs": "https://arxiv.org/abs/2511.21304", "authors": ["Luigi Catello", "Italo Napolitano", "Davide Salzano", "Mario di Bernardo"], "title": "Sparse shepherding control of large-scale multi-agent systems via Reinforcement Learning", "comment": null, "summary": "We propose a reinforcement learning framework for sparse indirect control of large-scale multi-agent systems, where few controlled agents shape the collective behavior of many uncontrolled agents. The approach addresses this multi-scale challenge by coupling ODEs (modeling controlled agents) with a PDE (describing the uncontrolled population density), capturing how microscopic control achieves macroscopic objectives. Our method combines model-free reinforcement learning with adaptive interaction strength compensation to overcome sparse actuation limitations. Numerical validation demonstrates effective density control, with the system achieving target distributions while maintaining robustness to disturbances and measurement noise, confirming that learning-based sparse control can replace computationally expensive online optimization."}
{"id": "2511.21310", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.21310", "abs": "https://arxiv.org/abs/2511.21310", "authors": ["Alailton J. Alves Junior", "Denis V. Coury", "Ricardo A. S. Fernandes"], "title": "Design and Performance Assessment of a Virtualized IED for Digital Substations", "comment": null, "summary": "Digital substations have significantly enhanced power grid protection by replacing traditional copper wiring with fiber-optic communication and integrating IEC 61850-compliant Intelligent Electronic Devices (IEDs), resulting in greater efficiency, reliability, and interoperability. While these advancements provide improved interoperability, challenges such as high costs, complex networks, and limited upgradeability persist. To mitigate these issues, the virtualization of IEDs has emerged as a cost-effective solution, offering scalability, simplified maintenance, and reduced hardware costs by replacing traditional hardware-based IEDs with software-based counterparts. However, the performance and reliability of virtual IEDs (vIED) must be rigorously evaluated to ensure their robustness in real-time applications. This paper develops, implements, and evaluates a vIED designed to match the performance of its hardware-based counterparts. The vIED was deployed on a server using virtual machines, with its core logic implemented in low-level programming languages to ensure high-speed, deterministic behavior. The performance was evaluated using real-time simulations, focusing on the response times of the protection functions. The results demonstrated that vIEDs achieved acceptable response times, validating their suitability for deployment in critical time-sensitive environments within digital substations."}
{"id": "2511.21314", "categories": ["eess.SY", "cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.21314", "abs": "https://arxiv.org/abs/2511.21314", "authors": ["Jewel Benny", "Narahari N. Moudhgalya", "Mujeev Khan", "Hemant Kumar Meena", "Mohd Wajid", "Abhishek Srivastava"], "title": "Scalable Multisubject Vital Sign Monitoring With mmWave FMCW Radar and FPGA Prototyping", "comment": "Published in IEEE Sensors Journal", "summary": "In this work, we introduce an innovative approach to estimate the vital signs of multiple human subjects simultaneously in a non-contact way using a Frequency Modulated Continuous Wave (FMCW) radar-based system. Traditional vital sign monitoring methods often face significant limitations, including subject discomfort with wearable devices, challenges in calibration, and the risk of infection transmission through contact measurement devices. To address these issues, this research is motivated by the need for versatile, non-contact vital monitoring solutions applicable in various critical scenarios. This work also explores the challenges of extending this capability to an arbitrary number of subjects, including hardware and theoretical limitations. Supported by rigorous experimental results and discussions, the paper illustrates the system's potential to redefine vital sign monitoring. An FPGA-based implementation is also presented as proof of concept for a hardware-based and portable solution, improving upon previous works by offering 2.7x faster execution and 18.4% less Look-Up Table (LUT) utilization, as well as providing over 7400x acceleration compared to its software counterpart."}
{"id": "2511.21319", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.21319", "abs": "https://arxiv.org/abs/2511.21319", "authors": ["Alailton J. Alves Junior", "Daniel Barbosa", "Ricardo A. S. Fernandes", "Denis V. Coury"], "title": "Analytical Phasor-Based Fault Location Enhancement for Wind Farm Collector Networks", "comment": null, "summary": "The increasing integration of Inverter-Based Resources (IBRs) is reshaping fault current characteristics, presenting significant challenges to traditional protection and fault location methods. This paper addresses a key limitation in fault location within wind farm collector networks, i.e., one-terminal phasor-based methods become inaccurate when IBRs are electrically located downstream from the fault. In such cases, the voltage drop caused by IBR fault current injections is not captured by the Intelligent Electronic Device, resulting in a systematic overestimation of fault distance. To mitigate this issue, a general compensation framework was proposed by augmenting classical loop formulations with a distance-dependent voltage correction term. The methodology was derived analytically using a sequence-domain representation and generalized to multiple fault types through a unified notation. It maintains the simplicity and interpretability of conventional approaches and can be implemented using only local measurements. The method was evaluated through EMT simulations in PSCAD using a realistic wind farm model. Results show significant improvements in location accuracy, with average and maximum errors notably reduced, especially for ground-involved faults where reductions exceed 90\\%. Furthermore, the compensation eliminates sensitivity to wind penetration levels and ensures uniform performance across feeders, positioning the method as a practical solution for modern renewable-dominated grids."}
{"id": "2511.21343", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.21343", "abs": "https://arxiv.org/abs/2511.21343", "authors": ["Laura Boca de Giuli", "Samuel Mallick", "Alessio La Bella", "Azita Dabiri", "Bart De Schutter", "Riccardo Scattolini"], "title": "Model Predictive Control and Moving Horizon Estimation using Statistically Weighted Data-Based Ensemble Models", "comment": "7 pages, 4 figures, submitted to ECC 2026", "summary": "This paper presents a model predictive control (MPC) framework leveraging an ensemble of data-based models to optimally control complex systems under multiple operating conditions. A novel combination rule for ensemble models is proposed, based on the statistical Mahalanobis distance, enabling the ensemble weights to suitably vary across the prediction window based on the system input. In addition, a novel state observer for ensemble models is developed using moving horizon estimation (MHE). The effectiveness of the proposed methodology is demonstrated on a benchmark energy system operating under multiple conditions."}
{"id": "2511.21371", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.21371", "abs": "https://arxiv.org/abs/2511.21371", "authors": ["Yichen Liu", "Hongyu Wu", "Bo Liu"], "title": "Evaluation of Large Language Models for Numeric Anomaly Detection in Power Systems", "comment": null, "summary": "Large language models (LLMs) have gained increasing attention in power grids for their general-purpose capabilities. Meanwhile, anomaly detection (AD) remains critical for grid resilience, requiring accurate and interpretable decisions based on multivariate telemetry. Yet the performance of LLMs on large-scale numeric data for AD remains largely unexplored. This paper presents a comprehensive evaluation of LLMs for numeric AD in power systems. We use GPT-OSS-20B as a representative model and evaluate it on the IEEE 14-bus system. A standardized prompt framework is applied across zero-shot, few-shot, in-context learning, low rank adaptation (LoRA), fine-tuning, and a hybrid LLM-traditional approach. We adopt a rule-aware design based on the three-sigma criterion, and report detection performance and rationale quality. This study lays the groundwork for further investigation into the limitations and capabilities of LLM-based AD and its integration with classical detectors in cyber-physical power grid applications."}
{"id": "2511.21385", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.21385", "abs": "https://arxiv.org/abs/2511.21385", "authors": ["Andrés E. Quintero", "Vinícius A. Lacerda", "Oriol Gomis-Bellmunt", "Moisés J. B. B. Davi", "Mario Oleskovicz"], "title": "Influence of converter current limiting and prioritization on protection of highly IBR-penetrated networks", "comment": "Submitted to DPSP Global 2026", "summary": "This paper investigates how grid-forming (GFM) and grid-following (GFL) control strategies in inverter-based resources (IBRs) influence line distance and differential protection in converter-dominated transmission systems. A modified IEEE 39-bus system is evaluated with GFM and GFL units equipped with low-voltage ride-through logic, current limiting, and positive- or negative-sequence prioritization. Distance protection is implemented with a mho characteristic, while line differential protection uses an alpha-plane approach. Results show that phase-to-ground loops in distance protection can substantially overestimate the fault location near the Zone-1 reach. For line differential protection, external faults may cause the operating point to briefly enter the trip region of the alpha-plane, even for the healthy-phase in ABG faults under GFL control and during the initial moments of the fault, demanding strong external security measures. These findings highlight that modern converter controls, together with current limitation and sequence-current prioritization, can compromise the reliability and security of traditional protection schemes."}
{"id": "2511.21387", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.21387", "abs": "https://arxiv.org/abs/2511.21387", "authors": ["Saurav Dulal", "Mohammed M. Olama", "Ali R. Ekti", "Nils M. Stenvig", "Yilu Liu"], "title": "Understanding Regional Inertia Dynamics in CAISO from Real Grid Disturbances", "comment": "This work has been accepted for publication in IEEE PES T&D 2026. The final published version will be available via IEEE Xplore", "summary": "The shift from synchronous generators to inverter-based resources has caused power system inertia to be unevenly distributed across power grids. As a result, certain grid regions are more vulnerable to high rate-of-change of frequency (RoCoF) during disturbances. This paper presents a measurement-based framework for estimating grid inertia in CAISO (California Independent System Operator) region using real disturbance-driven frequency data from the Frequency Monitoring Network (FNET/GridEye). By analyzing confirmed disturbances from 2013 to 2024, we identify trends in regional inertia and frequency dynamics, highlighting their relationship with renewable generation and the evolving duck curve. Regional RoCoF values were up to six times higher than interconnection-wide values, coinciding with declining inertia. Recent recovery in inertia is attributed to the increased deployment of battery energy storage systems with synthetic inertia capabilities. These findings underscore the importance of regional inertia monitoring, strategic resource planning, and adaptive operational practices to ensure grid reliability amid growing renewable integration."}
{"id": "2511.21405", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.21405", "abs": "https://arxiv.org/abs/2511.21405", "authors": ["Cristiana Punzo", "Italo Napolitano", "Cinzia Tomaselli", "Mario di Bernardo"], "title": "Decentralized Shepherding of Non-Cohesive Swarms Through Cluttered Environments via Deep Reinforcement Learning", "comment": null, "summary": "This paper investigates decentralized shepherding in cluttered environments, where a limited number of herders must guide a larger group of non-cohesive, diffusive targets toward a goal region in the presence of static obstacles. A hierarchical control architecture is proposed, integrating a high-level target assignment rule, where each herder is paired with a selected target, with a learning-based low-level driving module that enables effective steering of the assigned target. The low-level policy is trained in a one-herder-one-target scenario with a rectangular obstacle using Proximal Policy Optimization and then directly extended to multi-agent settings with multiple obstacles without requiring retraining. Numerical simulations demonstrate smooth, collision-free trajectories and consistent convergence to the goal region, highlighting the potential of reinforcement learning for scalable, model-free shepherding in complex environments."}
{"id": "2511.21432", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.21432", "abs": "https://arxiv.org/abs/2511.21432", "authors": ["Peter Iwer Hoedt Karstensen", "Roberto Galeazzi"], "title": "Multi-Hypotheses Navigation in Collaborative Localization subject to Cyber Attacks", "comment": null, "summary": "This paper addresses resilient collaborative localization in multi-agent systems exposed to spoofed radio frequency measurements. Each agent maintains multiple hypotheses of its own state and exchanges selected information with neighbors using covariance intersection. Geometric reductions based on distance tests and convex hull structure limit the number of hypotheses transmitted, controlling the spread of hypotheses through the network. The method enables agents to separate spoofed and truthful measurements and to recover consistent estimates once the correct hypothesis is identified. Numerical results demonstrate the ability of the approach to contain the effect of adversarial measurements, while also highlighting the impact of conservative fusion on detection speed. The framework provides a foundation for resilient multi-agent navigation and can be extended with coordinated hypothesis selection across the network."}
{"id": "2511.21456", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.21456", "abs": "https://arxiv.org/abs/2511.21456", "authors": ["Sagnik Ghosh", "Sandip Chakraborty"], "title": "VibraWave: Sensing the Pulse of Polluted Waters", "comment": null, "summary": "Conventional methods for water pollutant detection, such as chemical assays and optical spectroscopy, are often invasive, expensive, and unsuitable for real-time, portable monitoring. In this paper, we introduce VibraWave, a novel non-invasive sensing framework that combines mmWave radar with controlled acoustic excitation, tensor decomposition, and deep learning to detect and quantify a wide range of water pollutants. By capturing radar reflections as a three-dimensional tensor encoding phase dynamics, range bin power, and angle-of-arrival (AoA), we apply PARAFAC decomposition with non-negative constraints to extract compact, interpretable pollutant fingerprints. These are used to train a lightweight student neural network via knowledge distillation, enabling joint classification and quantification of heavy metals (Cu, Fe, Mg), oil emulsions, and sediments. Extensive experiments show that VibraWave achieves high accuracy and low RMSE across pure, binary, and tertiary mixtures, while remaining robust and computationally efficient, making it well-suited for scalable, real-time water quality monitoring."}
{"id": "2511.21619", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.21619", "abs": "https://arxiv.org/abs/2511.21619", "authors": ["Lorenzo Nespoli", "Vasco Medici"], "title": "Robust Rule-Based Sizing and Control of Batteries for Peak Shaving Applications", "comment": null, "summary": "As the cost of batteries lowers, sizing and control methods that are both fast and can achieve their promised performances when deployed are becoming more important. In this paper, we show how stochastically tuned rule based controllers (RBCs) can be effectively used to achieve both these goals, providing more realistic estimates in terms of achievable levelised cost of energy (LCOE), and better performances while in operation when compared to deterministic model predictive control (MPC). We test the proposed methodology on yearly profiles from real meters for peak shaving applications and provide strong evidence about these claims."}
{"id": "2511.21633", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.21633", "abs": "https://arxiv.org/abs/2511.21633", "authors": ["Liraz Mudrik", "Yaakov Oshman"], "title": "Bang-Bang Evasion: Its Stochastic Optimality and a Terminal-Set-Based Implementation", "comment": "29 pages, 4 figures", "summary": "We address the problem of optimal evasion in a planar endgame engagement, where a target with bounded lateral acceleration seeks to avoid interception by a missile guided by a linear feedback law. Contrary to existing approaches, that assume perfect information or use heuristic maneuver models in stochastic settings, we formulate the problem in an inherently stochastic framework involving imperfect information and bounded controls. Complying with the generalized separation theorem, the control law factors in the posterior distribution of the state. Extending the well-known optimality of bang-bang evasion maneuvers in deterministic settings to the realm of realistic, stochastic evasion scenarios, we firstly prove that an optimal evasion strategy always exists, and that the set of optimal solutions includes at least one bang-bang policy, rendering the resulting optimal control problem finite-dimensional. Leveraging this structure, we secondly propose the closed-loop terminal-set-based evasion (TSE) strategy, and demonstrate its effectiveness in simulation against a proportional navigation pursuer. Monte Carlo simulations show that the TSE strategy outperforms traditional stochastic evasion strategies based on random telegraph, Singer, and weaving models."}
{"id": "2511.21641", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.21641", "abs": "https://arxiv.org/abs/2511.21641", "authors": ["Michael Ruderman"], "title": "Model-free practical PI-Lead control design by ultimate sensitivity principle", "comment": "6 pages, 10 figures", "summary": "Practical design and tuning of feedback controllers has to do often without any model of the given dynamic process. Only some general assumptions about the process, in this work type-one stable behavior, can be available for engineers, in particular in motion control systems. This paper proposes a practical and simple in realization procedure for designing a robust PI-Lead control without modeling. The developed method derives from the ultimate sensitivity principles, known in the empirical Ziegler-Nichols tuning of PID control, and makes use of some general characteristics of loop shaping. A three-steps procedure is proposed to determine the integration time constant, control gain, and Lead-element in a way to guarantee a sufficient phase margin, while all steps are served by only experimental observations of the output value. The proposed method is also evaluated with experiments on a noise-perturbed electro-mechanical actuator system with translational motion."}
{"id": "2511.20709", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.20709", "abs": "https://arxiv.org/abs/2511.20709", "authors": ["Abhijeet Pathak", "Suvadra Barua", "Dinesh Gudimetla", "Rupam Patir", "Jiawei Guo", "Hongxin Hu", "Haipeng Cai"], "title": "DUALGUAGE: Automated Joint Security-Functionality Benchmarking for Secure Code Generation", "comment": null, "summary": "Large language models (LLMs) and autonomous coding agents are increasingly used to generate software across a wide range of domains. Yet a core requirement remains unmet: ensuring that generated code is secure without compromising its functional correctness. Existing benchmarks and evaluations for secure code generation fall short-many measure only vulnerability reduction, disregard correctness preservation, or evaluate security and functionality on separate datasets, violating the fundamental need for simultaneous joint evaluation. We present DUALGAUGE, the first fully automated benchmarking framework designed to rigorously evaluate the security and correctness of LLM-generated code in unison. Given the lack of datasets enabling joint evaluation of secure code generation, we also present DUALGAUGE-BENCH, a curated benchmark suite of diverse coding tasks, each paired with manually validated test suites for both security and functionality, designed for full coverage of specification requirements. At the core of DUALGAUGE is an agentic program executor, which runs a program against given tests in sandboxed environments, and an LLM-based evaluator, which assesses both correctness and vulnerability behavior against expected outcomes. We rigorously evaluated and ensured the quality of DUALGAUGE-BENCH and the accuracy of DUALGAUGE, and applied DUALGAUGE to benchmarking ten leading LLMs on DUALGAUGE-BENCH across thousands of test scenarios. Our results reveal critical gaps in correct and secure code generation by these LLMs, for which our open-source system and datasets help accelerate progress via reproducible, scalable, and rigorous evaluation."}
{"id": "2511.20730", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20730", "abs": "https://arxiv.org/abs/2511.20730", "authors": ["Nehal Afifi", "Christoph Wittig", "Lukas Paehler", "Andreas Lindenmann", "Kai Wolter", "Felix Leitenberger", "Melih Dogru", "Patric Grauberger", "Tobias Düser", "Albert Albers", "Sven Matthiesen"], "title": "Data-Driven Methods and AI in Engineering Design: A Systematic Literature Review Focusing on Challenges and Opportunities", "comment": null, "summary": "The increasing availability of data and advancements in computational intelligence have accelerated the adoption of data-driven methods (DDMs) in product development. However, their integration into product development remains fragmented. This fragmentation stems from uncertainty, particularly the lack of clarity on what types of DDMs to use and when to employ them across the product development lifecycle. To address this, a necessary first step is to investigate the usage of DDM in engineering design by identifying which methods are being used, at which development stages, and for what application. This paper presents a PRISMA systematic literature review. The V-model as a product development framework was adopted and simplified into four stages: system design, system implementation, system integration, and validation. A structured search across Scopus, Web of Science, and IEEE Xplore (2014--2024) retrieved 1{,}689 records. After screening, 114 publications underwent full-text analysis. Findings show that machine learning (ML) and statistical methods dominate current practice, whereas deep learning (DL), though still less common, exhibits a clear upward trend in adoption. Additionally, supervised learning, clustering, regression analysis, and surrogate modeling are prevalent in design, implementation, and integration system stages but contributions to validation remain limited. Key challenges in existing applications include limited model interpretability, poor cross-stage traceability, and insufficient validation under real-world conditions. Additionally, it highlights key limitations and opportunities such as the need for interpretable hybrid models. This review is a first step toward design-stage guidelines; a follow-up synthesis should map computer science algorithms to engineering design problems and activities."}
{"id": "2511.20955", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20955", "abs": "https://arxiv.org/abs/2511.20955", "authors": ["Sanchit Kaul", "Kevin Nhu", "Jason Eissayou", "Ivan Eser", "Victor Borup"], "title": "SpaceX: Exploring metrics with the SPACE model for developer productivity", "comment": "Code available at https://github.com/knhu/ECS260Project", "summary": "This empirical investigation elucidates the limitations of deterministic, unidimensional productivity heuristics by operationalizing the SPACE framework through extensive repository mining. Utilizing a dataset derived from open-source repositories, the study employs rigorous statistical methodologies including Generalized Linear Mixed Models (GLMM) and RoBERTa-based sentiment classification to synthesize a holistic, multi-faceted productivity metric. Analytical results reveal a statistically significant positive correlation between negative affective states and commit frequency, implying a cycle of iterative remediation driven by frustration. Furthermore, the investigation has demonstrated that analyzing the topology of contributor interactions yields superior fidelity in mapping collaborative dynamics compared to traditional volume-based metrics. Ultimately, this research posits a Composite Productivity Score (CPS) to address the heterogeneity of developer efficacy."}
{"id": "2511.21121", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21121", "abs": "https://arxiv.org/abs/2511.21121", "authors": ["Anup Roy", "Rishabh Gyanendra Upadhyay", "Animesh Rameshbhai Panara", "Robin Mills"], "title": "Beyond Patch Aggregation: 3-Pass Pyramid Indexing for Vision-Enhanced Document Retrieval", "comment": null, "summary": "Document centric RAG pipelines usually begin with OCR, followed by brittle heuristics for chunking, table parsing, and layout reconstruction. These text first workflows are costly to maintain, sensitive to small layout shifts, and often lose the spatial cues that contain the answer. Vision first retrieval has emerged as a strong alternative. By operating directly on page images, systems like ColPali and ColQwen preserve structure and reduce pipeline complexity while achieving strong benchmark performance. However, these late interaction models tie retrieval to a specific vision backbone and require storing hundreds of patch embeddings per page, creating high memory overhead and complicating large scale deployment.\n  We introduce VisionRAG, a multimodal retrieval system that is OCR free and model agnostic. VisionRAG indexes documents directly as images, preserving layout, tables, and spatial cues, and builds semantic vectors without committing to a specific extraction. Our three pass pyramid indexing framework creates vectors using global page summaries, section headers, visual hotspots, and fact level cues. These summaries act as lightweight retrieval surrogates. At query time, VisionRAG retrieves the most relevant pages using the pyramid index, then forwards the raw page image encoded as base64 to a multimodal LLM for final question answering. During retrieval, reciprocal rank fusion integrates signals across the pyramid to produce robust ranking.\n  VisionRAG stores only 17 to 27 vectors per page, matching the efficiency of patch based methods while staying flexible across multimodal encoders. On financial document benchmarks, it achieves 0.8051 accuracy at 10 on FinanceBench and 0.9629 recall at 100 on TAT DQA. These results show that OCR free, summary guided multimodal retrieval is a practical and scalable alternative to traditional text extraction pipelines."}
{"id": "2511.21389", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21389", "abs": "https://arxiv.org/abs/2511.21389", "authors": ["Guoxiao Zhang", "Ao Li", "Tan Qu", "Qianlong Xie", "Xingxing Wang"], "title": "FITRep: Attention-Guided Item Representation via MLLMs", "comment": null, "summary": "Online platforms usually suffer from user experience degradation due to near-duplicate items with similar visuals and text. While Multimodal Large Language Models (MLLMs) enable multimodal embedding, existing methods treat representations as black boxes, ignoring structural relationships (e.g., primary vs. auxiliary elements), leading to local structural collapse problem. To address this, inspired by Feature Integration Theory (FIT), we propose FITRep, the first attention-guided, white-box item representation framework for fine-grained item deduplication. FITRep consists of: (1) Concept Hierarchical Information Extraction (CHIE), using MLLMs to extract hierarchical semantic concepts; (2) Structure-Preserving Dimensionality Reduction (SPDR), an adaptive UMAP-based method for efficient information compression; and (3) FAISS-Based Clustering (FBC), a FAISS-based clustering that assigns each item a unique cluster id using FAISS. Deployed on Meituan's advertising system, FITRep achieves +3.60% CTR and +4.25% CPM gains in online A/B tests, demonstrating both effectiveness and real-world impact."}
{"id": "2511.21394", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21394", "abs": "https://arxiv.org/abs/2511.21394", "authors": ["Guoxiao Zhang", "Tan Qu", "Ao Li", "DongLin Ni", "Qianlong Xie", "Xingxing Wang"], "title": "RIA: A Ranking-Infused Approach for Optimized listwise CTR Prediction", "comment": null, "summary": "Reranking improves recommendation quality by modeling item interactions. However, existing methods often decouple ranking and reranking, leading to weak listwise evaluation models that suffer from combinatorial sparsity and limited representational power under strict latency constraints. In this paper, we propose RIA (Ranking-Infused Architecture), a unified, end-to-end framework that seamlessly integrates pointwise and listwise evaluation. RIA introduces four key components: (1) the User and Candidate DualTransformer (UCDT) for fine-grained user-item-context modeling; (2) the Context-aware User History and Target (CUHT) module for position-sensitive preference learning; (3) the Listwise Multi-HSTU (LMH) module to capture hierarchical item dependencies; and (4) the Embedding Cache (EC) module to bridge efficiency and effectiveness during inference. By sharing representations across ranking and reranking, RIA enables rich contextual knowledge transfer while maintaining low latency. Extensive experiments show that RIA outperforms state-of-the-art models on both public and industrial datasets, achieving significant gains in AUC and LogLoss. Deployed in Meituan advertising system, RIA yields a +1.69% improvement in Click-Through Rate (CTR) and a +4.54% increase in Cost Per Mille (CPM) in online A/B tests."}
{"id": "2511.21413", "categories": ["cs.DC", "cs.AI", "cs.DB", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.21413", "abs": "https://arxiv.org/abs/2511.21413", "authors": ["Tim Trappen", "Robert Keßler", "Roland Pabel", "Viktor Achter", "Stefan Wesner"], "title": "Automated Dynamic AI Inference Scaling on HPC-Infrastructure: Integrating Kubernetes, Slurm and vLLM", "comment": "6 pages, 3 figures", "summary": "Due to rising demands for Artificial Inteligence (AI) inference, especially in higher education, novel solutions utilising existing infrastructure are emerging. The utilisation of High-Performance Computing (HPC) has become a prevalent approach for the implementation of such solutions. However, the classical operating model of HPC does not adapt well to the requirements of synchronous, user-facing dynamic AI application workloads. In this paper, we propose our solution that serves LLMs by integrating vLLM, Slurm and Kubernetes on the supercomputer \\textit{RAMSES}. The initial benchmark indicates that the proposed architecture scales efficiently for 100, 500 and 1000 concurrent requests, incurring only an overhead of approximately 500 ms in terms of end-to-end latency."}
